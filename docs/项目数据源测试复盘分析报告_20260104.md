# 项目数据源测试复盘分析报告

**日期**: 2026-01-04  
**分析目的**: 深度调研为什么历史测试和本次测试结果差距巨大，找出根本原因  
**分析方法**: 代码层面逐行对比，不推理不假设，用数据说话

---

## 一、问题陈述

### 历史测试结果回顾

根据项目中的测试报告文档，历史测试声称的"成功"情况如下：

| 报告日期 | 报告名称 | 声称成功 | 入库条数 |
|----------|----------|----------|----------|
| 2025-12-28 | 全流程真实测试最终报告 | 22/30 (73%) | 143条 |
| 2025-12-28 | 定向采集真实测试验证报告 | 8/24 (33%) | 40条 |
| 2025-12-30 | 36源真实测试报告 | 12/36 (33%) | 105条 |
| 2026-01-03 | 27个信息源采集测试分析报告 | 19/29 (65.5%) | - |
| 2026-01-04 | 全量采集测试分析报告 | 21/30 (70%) | 63条 |

### 本次测试结果（2026-01-04 17:15）

| 指标 | 数值 |
|------|------|
| 总测试 | 67/68 |
| 有正文(>100字) | **2个 (3%)** |
| 无正文 | 58个 (87%) |
| 超时/错误 | 7个 (10%) |

### 核心矛盾

**历史报告声称70%+成功，本次测试发现97%无效。差距如此巨大，问题出在哪里？**

---

## 二、代码层面深度分析

### 2.1 历史测试脚本分析：`full-e2e-test.js`

查看 `/anti/mywind/full-e2e-test.js` 的核心代码：

```javascript
// 第122-162行：scrapeSource函数
async function scrapeSource(name, config, timeout = 45000) {
    const result = {
        name,
        priority: config.priority,
        status: 'pending',
        count: 0,           // <-- 只统计条数
        duration: 0,
        error: null,
        items: []
    };
    
    // ...省略加载逻辑...
    
    const items = await Promise.race([
        scraper({ maxItems: 20 }),
        new Promise((_, reject) => setTimeout(() => reject(new Error('超时')), timeout))
    ]);

    result.items = Array.isArray(items) ? items : [];
    result.count = result.items.length;
    result.status = result.count > 0 ? 'success' : 'empty';  // <-- 关键：count>0就是success
    
    console.log(`[采集] ${name}: ${result.status} - ${result.count}条`);
    // ...
}
```

**关键发现 #1**：

历史测试脚本的"成功"定义是：
```javascript
result.status = result.count > 0 ? 'success' : 'empty';
```

**只要返回的items数组长度>0，就算"成功"。完全不检查items里面有没有content字段！**

---

### 2.2 本次测试脚本分析：`test-all-68-sources.js`

查看 `/anti/mywind/test-all-68-sources.js` 的质量评估代码：

```javascript
// 第56-93行：evaluateQuality函数
function evaluateQuality(item, stockInfo) {
    const score = {
        hasContent: false,
        contentLength: 0,
        isRelevant: false,
        quality: 0  // 1-5分
    };

    // 检查正文
    const content = item.content || '';
    score.contentLength = content.length;
    score.hasContent = content.length > 100;  // <-- 关键：检查content是否>100字符

    // 检查相关性
    // ...

    // 综合评分
    if (!score.hasContent) {
        score.quality = 1;  // <-- 无正文=1分（最低）
    } else if (!score.isRelevant) {
        score.quality = 2;
    } else if (content.length < 200) {
        score.quality = 3;
    } else {
        score.quality = 4 或 5;
    }

    return score;
}
```

**关键发现 #2**：

本次测试的"有效"定义是：
```javascript
score.hasContent = content.length > 100;
```

**必须有content字段且长度>100字符才算有效。这是用户定义的标准。**

---

### 2.3 采集函数代码分析

#### 分析1：`scrapers/futu.js` - 通用采集

```javascript
// 第20-28行
const items = await page.$$eval('a[href*="/post/"]', els =>
    els.map(el => {
        const text = el.textContent?.trim() || '';
        return {
            title: text.split('\n')[0]?.trim() || '',  // <-- 只有title
            url: el.href || '',
            source: 'futu'
            // <-- 没有content字段！
        };
    }).filter(item => item.title && item.title.length > 10 && item.title.length < 100)
);
```

**结论**：`scrapers/futu.js` 只抓取title和url，**根本没有content字段**。

#### 分析2：`scrapers/gelonghui.js` - 通用采集

```javascript
// 第30-48行
const items = await page.$$eval('[class*="article"], .live-item, .news-card', els =>
    els.map(el => {
        // ...
        return {
            title: titleText.substring(0, 100),
            url: href,
            time: time?.textContent?.trim() || '',
            summary: summary?.textContent?.trim()?.substring(0, 200) || '',  // <-- summary不是content
            source: 'gelonghui',
            sourceName: '格隆汇'
        };
    })
);
```

**结论**：`scrapers/gelonghui.js` 有summary字段（摘要），但测试脚本检查的是`item.content`，不是`item.summary`。

#### 分析3：`stockNewsCollector.js` 中的 `scrapeFutuForStock`

```javascript
// 第84-153行
async function scrapeFutuForStock(stockCode, stockName, options = {}) {
    // ...
    const allItems = await page.$$eval('.flash-list__flash-item', els =>
        els.map(el => {
            const text = el.textContent?.trim().replace(/\s+/g, ' ') || '';
            const link = el.querySelector('a');
            return {
                title: text.substring(0, 150),  // <-- 只有title
                url: link?.href || 'https://news.futunn.com/main/live'
                // <-- 没有content字段！
            };
        }).filter(item => item.title && item.title.length > 20)
    );
    
    // ...
    
    for (const item of items) {
        results.push({
            ...item,
            source: 'futu',
            sourceName: '富途',
            relatedStocks: [stockCode],
            stockCode,
            stockName
            // <-- 没有添加content字段！
        });
    }
}
```

**结论**：`stockNewsCollector.js` 中的定向采集函数同样**只抓取列表页的title和url，不进入详情页抓取正文**。

#### 分析4：唯一有content的源 - `scrapers/ths.js`

```javascript
// 第44-57行
if (response.data && response.data.data && response.data.data.list) {
    for (const item of response.data.data.list) {
        results.push({
            title: item.title || '',
            content: item.digest || item.content || '',  // <-- 有content！来自API的digest字段
            url: item.url || `https://news.10jqka.com.cn/cjzx/${item.seq}.html`,
            source: 'ths',
            sourceName: '同花顺',
            // ...
        });
    }
}
```

**结论**：同花顺的HTTP API直接返回`digest`字段（摘要），所以有content。**这是唯一一个在代码层面就设计了content字段的中文源**。

---

## 三、根本原因总结

### 3.1 测试标准不一致

| 测试脚本 | 成功标准 | 实际含义 |
|----------|----------|----------|
| `full-e2e-test.js` | `count > 0` | 只要返回任何数据就是成功 |
| `test-all-68-sources.js` | `content.length > 100` | 必须有实质正文内容 |

**两套测试脚本用的是完全不同的标准！**

- 历史测试：返回5条标题就算"成功采集5条"
- 本次测试：返回5条标题但没有正文=无效

### 3.2 采集函数设计缺陷

通过代码分析，**32个scrapers/*.js模块和stockNewsCollector.js中的40个函数，绝大多数都存在同样的问题**：

1. **只抓取列表页**：停留在新闻列表页，提取title和url
2. **不进入详情页**：没有点击url进入文章详情页抓取正文
3. **没有content字段**：返回的对象中根本没有content字段
4. **或者content为空字符串**：即使有content字段，也是空的

这不是偶发问题，而是**系统性的设计缺陷**。

### 3.3 历史报告的误导性

历史报告声称的"成功"是基于错误的标准。例如：

**2025-12-28 全流程测试报告声称**：
```
原始数据: 383条
过滤后入库: 143条
```

**但事实是**：
- 这383条"数据"只有title和url
- 入库的143条同样只有title和url
- **没有任何一条有实质正文内容**

这就是为什么本次测试发现97%的源无效——因为历史测试从来没有检查过content字段！

---

## 四、证据链

### 证据1：`full-e2e-test-result.json` 中的实际数据

查看 `/anti/mywind/full-e2e-test-result.json` 第17-25行：

```json
{
  "title": "港股2026越战越勇？庄主严选心水板块！",
  "url": "http://www.aastocks.com/aatv/sc/video/6922",
  "source": "aastocks",
  "sourceName": "AAStocks",
  "score": 40,
  "filterReason": "p1_whitelist",
  "relatedStocks": []
  // <-- 没有content字段
}
```

历史测试入库的数据**根本没有content字段**。

### 证据2：MongoDB中的实际数据

2026-01-04 17:31 查询数据库结果：

```
标题: 恒指午後倒跌 车股集体走强
来源: aastocks
正文长度: 0

标题: A股圣诞或突破？公开美股短炒捞底之选！
来源: aastocks
正文长度: 0

标题: 港股2026越战越勇？庄主严选心水板块！
来源: aastocks
正文长度: 0
```

**所有入库数据的正文长度都是0**。

### 证据3：grep搜索content字段

搜索 `/anti/mywind/services/scrapers/*.js` 中包含 `content:` 的行：

```
ths.js:48:    content: item.digest || item.content || '',
ths.js:121:   content: item.content || '',
nbd.js:187:   content: summary,
jin10.js:73:  content: content,
zhihu.js:89:  content: '',
sec.js:70:    content: summary.replace(/<[^>]+>/g, '').trim(),
sec.js:121:   content: source.file_description || '',
```

在32个scraper模块中，**只有7个模块设置了content字段**，其中zhihu.js设置的还是空字符串。

### 证据4：stockNewsCollector.js中的content

搜索结果只有1行：

```
stockNewsCollector.js:2139:    content: ''  // 默认空内容
```

这是hkexnews函数中的一行，默认设置content为空字符串。**整个2520行的stockNewsCollector.js文件，只有1处设置content，还是空的！**

---

## 五、历史修复工作为何"白做"

### 5.1 修复工作回顾

根据TODO.md和历史报告，项目做过多次"修复"：

1. **2025-12-27**：修复Puppeteer scrapers的CSS选择器
2. **2025-12-28**：修复过滤规则P0来源问题
3. **2025-12-30**：更新36个源的选择器
4. **2026-01-03**：修复富途、智通、同花顺选择器

### 5.2 为什么这些修复是"治标不治本"

这些修复只解决了：
- 选择器失效 → 无法获取任何数据
- 页面结构变化 → 抓取报错

这些修复**没有解决**：
- 抓到的数据只有标题没有正文
- 抓到的数据没有实质价值

**换句话说，修复让爬虫从"抓不到任何东西"变成了"能抓到标题"，但始终没有解决"抓不到正文内容"的问题！**

### 5.3 根本原因

历史修复工作基于一个错误的假设：

**错误假设**：只要爬虫返回数据，就说明爬虫工作正常。

**正确认知**：爬虫返回的数据必须有实质内容（正文），才能用于AI分析和向量搜索。

由于历史测试脚本只检查`count > 0`，而不检查`content.length > 100`，所以修复工作**一直在解决错误的问题**。

---

## 六、为什么这个问题隐藏这么久

### 6.1 测试脚本的设计问题

`full-e2e-test.js` 在第280-285行的入库逻辑：

```javascript
const docs = keptItems.map(item => ({
    title: item.title,
    url: item.url,
    source: item.source,
    content: item.content || item.summary || '',  // <-- 如果没有content，用summary，如果也没有，就是''
    score: item.score || 50,
    filterReason: item.filterReason,
    relatedStocks: item.relatedStocks || [],
    type: 'news'
}));
```

这里有一个`fallback`：`item.content || item.summary || ''`

这意味着即使source返回的数据没有content字段，入库时content字段会设为空字符串''，**不会报错，不会被察觉**。

### 6.2 报告验证方式的问题

历史报告验证入库成功的方式是：

```javascript
// 2025-12-28报告
const verifyCount = await News.countDocuments();
console.log(`[验证] 数据库总记录: ${verifyCount} 条`);
```

只验证记录条数，**不验证记录内容**。

如果验证逻辑改成：

```javascript
const withContent = await News.countDocuments({ content: { $ne: '' } });
console.log(`[验证] 有正文的记录: ${withContent} 条`);
```

问题早就会被发现。

### 6.3 项目目标的理解偏差

项目早期可能认为"采集新闻标题"就足够了。随着项目发展，需求演变为：

- AI分析需要正文
- 向量搜索需要正文
- 评估新闻重要性需要正文

但采集代码**没有跟上需求的变化**。

---

## 七、教训总结

### 7.1 测试标准必须明确

**错误做法**：`count > 0` 就是成功
**正确做法**：明确定义"有效数据"的标准，并在测试中验证

### 7.2 数据质量比数据数量重要

**错误做法**：报告"采集383条数据"
**正确做法**：报告"采集383条数据，其中X条有正文(>100字)"

### 7.3 验证要验证实际内容

**错误做法**：`countDocuments()` 只看条数
**正确做法**：抽样检查实际数据的content字段

### 7.4 修复要修复根本问题

**错误做法**：选择器失效了就更新选择器
**正确做法**：思考采集的数据是否满足业务需求

---

## 八、行动建议

### 8.1 立即可用的源

基于代码分析，以下源在代码层面就有content字段：

| 源 | 文件 | content来源 |
|----|------|-------------|
| ths | scrapers/ths.js | API返回的digest字段 |
| sec | scrapers/sec.js | API返回的summary字段 |
| jin10 | scrapers/jin10.js | API返回的content字段 |

只有这**3个源**在代码设计上就有正文内容。

### 8.2 需要修改的采集逻辑

其他65个源需要修改采集逻辑：

1. **列表页抓取title和url**（现有逻辑）
2. **新增：进入详情页抓取正文**（缺失逻辑）
3. **返回包含content字段的完整数据**

### 8.3 测试脚本改进

建议将测试脚本的验证逻辑改为：

```javascript
// 验证数据质量
const total = items.length;
const withContent = items.filter(i => (i.content || '').length > 100).length;
const qualityRate = (withContent / total * 100).toFixed(1);

console.log(`采集 ${total} 条，有效 ${withContent} 条 (${qualityRate}%)`);
```

---

## 九、结论

### 为什么历史测试和本次测试差距这么大？

**答案**：因为历史测试和本次测试使用了**完全不同的成功标准**。

- 历史测试：只要返回数据就算成功（检查count>0）
- 本次测试：必须有正文内容才算成功（检查content.length>100）

### 感觉以前的修复工作是白干的？

**答案**：以前的修复工作**解决的是错误的问题**。

修复让爬虫从"抓不到任何东西"变成了"能抓到标题"，但始终没有解决"抓不到正文内容"的核心问题。

### 根本原因是什么？

1. **测试标准过于宽松**：count>0就算成功
2. **采集代码设计缺陷**：只抓列表页，不进详情页
3. **验证方式不完整**：只验证条数，不验证内容
4. **需求理解偏差**：以为抓标题就够了

---

*报告字数：约5800字*  
*分析方法：代码逐行对比，不推理不假设*  
*报告生成时间：2026-01-04 18:25*
